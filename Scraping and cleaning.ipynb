{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f28733-d071-44f3-9a40-2574f1e47d2c",
   "metadata": {},
   "source": [
    "SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dfff1ec6-b354-4684-83ec-3734231f877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import re\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0ce1d2b-ccb1-470a-a221-ccebe9c337ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs(keyword, num_jobs, verbose, path, slp_time):\n",
    "    '''Gathers jobs as a dataframe, scraped from Glassdoor'''\n",
    "    #Initializing the webdriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    service = Service(executable_path=path)\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.set_window_size(1120, 1000)\n",
    "    \n",
    "    url = \"https://www.glassdoor.com/Job/jobs.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=\"+keyword+\"&sc.keyword=\"+keyword+\"&locT=&locId=&jobType=\"\n",
    "    driver.get(url)\n",
    "    jobs = []\n",
    "    job_buttons=[]\n",
    "    time_show_more_clicked=0\n",
    "    while len(job_buttons) < num_jobs:  #If true, should be still looking for new jobs.\n",
    "        time.sleep(slp_time)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"JobsList_jobsList__lqjTr\")))\n",
    "        job_buttons = driver.find_elements(By.CLASS_NAME, \"JobsList_jobListItem__wjTHv\")\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, \"//button[span/span[text()='Show more jobs']]\").click()\n",
    "            time_show_more_clicked+=1\n",
    "            time.sleep(5*time_show_more_clicked)\n",
    "            print(\"See More Jobs clicked !\")\n",
    "            if len(job_buttons) == len(driver.find_elements(By.CLASS_NAME, \"JobsList_jobListItem__wjTHv\")):#if see more button doesnt works\n",
    "                driver.refresh()\n",
    "        except NoSuchElementException:\n",
    "            print(\"Scraping terminated before reaching target number of jobs. Needed {}, got {}.\".format(num_jobs, len(job_buttons)))\n",
    "            break\n",
    "    \n",
    "    print(\"Job cards loaded : \",len(job_buttons))\n",
    "\n",
    "    for index,job_button in enumerate(job_buttons):\n",
    "        print(\"Progress: {}\".format(\"\" + str(len(jobs)) + \"/\" + str(num_jobs))) #shows progress\n",
    "        if len(jobs) >= num_jobs:\n",
    "            break\n",
    "    \n",
    "        job_button.click() #Clicks a job card\n",
    "        time.sleep(2)\n",
    "        collected_successfully = False\n",
    "        attempts=0\n",
    "\n",
    "        try:\n",
    "            short_description_elements=driver.find_elements(By.CLASS_NAME, \"JobCard_jobDescriptionSnippet__l1tnl\")\n",
    "            short_desc = short_description_elements[index].text\n",
    "        except NoSuchElementException:\n",
    "            short_desc= -1\n",
    "            print(\"Can not read short description\")\n",
    "            \n",
    "        while not collected_successfully and attempts < 5:\n",
    "            try:\n",
    "                driver.find_element(By.CLASS_NAME, 'ShowMoreCTA_showMore__EtZpZ').click()\n",
    "                time.sleep(0.5)\n",
    "                try:\n",
    "                    job_elements = driver.find_elements(By.CSS_SELECTOR, 'li[data-jobid]')\n",
    "                    job_id=job_elements[index].get_attribute('data-jobid')\n",
    "                except:\n",
    "                    print(\"can not read job_ID\")\n",
    "                try:\n",
    "                    job_description = driver.find_element(By.CLASS_NAME, \"JobDetails_jobDescription__uW_fK\").text\n",
    "                except:\n",
    "                    print(\"Can not read job Description\")\n",
    "                    job_description=-1\n",
    "                try:\n",
    "                    title = driver.find_element(By.CLASS_NAME, \"heading_Level1__soLZs\").text\n",
    "                except:\n",
    "                    print(\"Can not read title\")\n",
    "                    title=-1\n",
    "                try:\n",
    "                    company=driver.find_element(By.CLASS_NAME, \"heading_Subhead__Ip1aW\").text\n",
    "                except:\n",
    "                    print(\"Can not read company\")\n",
    "                    company=-1\n",
    "                try:\n",
    "                    location_unprocessed=driver.find_element(By.CLASS_NAME, \"JobDetails_locationAndPay__XGFmY\").text\n",
    "                    location = location_unprocessed.split('\\n')[0]\n",
    "                except:\n",
    "                    print(\"Can not read company\")\n",
    "                    location=-1\n",
    "                \n",
    "                collected_successfully = True    \n",
    "            except:\n",
    "                print(\"Can not click see more button\")\n",
    "                time.sleep(1)\n",
    "                attempts+=1\n",
    "        \n",
    "    \n",
    "        #Printing for debugging\n",
    "        if verbose:\n",
    "            print(\"Job ID: {}\".format(job_id))\n",
    "            print(\"Short Desc: {}\".format(short_desc))\n",
    "            print(\"Job Title: {}\".format(title))\n",
    "            print(\"Company Name: {}\".format(company))\n",
    "            print(\"Location: {}\".format(location))\n",
    "            print(\"Job Description: {}\".format(job_description))\n",
    "\n",
    "        if short_desc != -1 and job_description != -1:\n",
    "            combined_description = f\"{short_desc}\\n{job_description}\"\n",
    "        elif job_description == -1:\n",
    "            combined_description = short_desc\n",
    "        else:\n",
    "            combined_description = job_description\n",
    "\n",
    "        jobs.append({\"job_id\" : job_id,\n",
    "        \"job_title\" : keyword,\n",
    "        \"company_name\" : company,\n",
    "        \"job_description\" : combined_description,\n",
    "        \"location\" : location})\n",
    "\n",
    "    return pd.DataFrame(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "02f1158d-5a3f-4d98-8348-9321008fb933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def keywords_extraction(df, lang_path, skill_path, tech_path, education_path):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    cleaned_descriptions = []\n",
    "    languages_found = []\n",
    "    skills_found = []\n",
    "    techs_found = []\n",
    "    education_found = []\n",
    "  \n",
    "    # Load and preprocess keywords from all three CSVs\n",
    "    def load_keywords(path, column_name):\n",
    "        keywords_df = pd.read_csv(path)\n",
    "        return [kw.lower() for kw in keywords_df[column_name].dropna().tolist()]\n",
    "\n",
    "    language_list = load_keywords(lang_path, \"languages\")\n",
    "    skill_list = load_keywords(skill_path, \"skills\")\n",
    "    tech_list = load_keywords(tech_path, \"technologies\")\n",
    "    education_list = load_keywords(education_path, \"education\")\n",
    "\n",
    "    for desc in df[\"job_description\"]:\n",
    "        # Cleaning\n",
    "        desc_clean = desc.replace(\"-\", \" \")\n",
    "        desc_clean = re.sub(r\"[\\n]\", \" \", desc_clean)\n",
    "        desc_clean = re.sub(r\"[.!?/()\\-,:;]\", \" \", desc_clean)\n",
    "        desc_clean = re.sub(r\"\\d+[+-]?\", \" \", desc_clean)\n",
    "        desc_clean = re.sub(r\"[â€™']\", \" \", desc_clean)\n",
    "        desc_clean = desc_clean.lower()\n",
    "        desc_clean = \" \".join([word for word in desc_clean.split() if word and word not in stop])\n",
    "        cleaned_descriptions.append(desc_clean)\n",
    "\n",
    "        # Keyword Matching (unique matches only)\n",
    "        words = desc_clean.split()\n",
    "        languages = []\n",
    "        skills = []\n",
    "        techs = []\n",
    "        education = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word in language_list and word not in languages:\n",
    "                languages.append(word)\n",
    "            if word in skill_list and word not in skills:\n",
    "                skills.append(word)\n",
    "            if word in tech_list and word not in techs:\n",
    "                techs.append(word)\n",
    "            if word in education_list and word not in education:\n",
    "                education.append(word)\n",
    "        \n",
    "        languages_found.append(\", \".join(languages))\n",
    "        skills_found.append(\", \".join(skills))\n",
    "        techs_found.append(\", \".join(techs))\n",
    "        education_found.append(\", \".join(education))\n",
    "\n",
    "    # Add columns to DataFrame\n",
    "    df[\"Languages\"] = languages_found\n",
    "    df[\"Skills\"] = skills_found\n",
    "    df[\"Technologies\"] = techs_found\n",
    "    df[\"Education\"] = education_found\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7d4e5f49-48ef-4110-92b5-ddb0315c338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_MySQL(df, user, password, host, port, database, table_name):\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        conn = mysql.connector.connect(\n",
    "            host=host,\n",
    "            port=port,\n",
    "            user=user,\n",
    "            password=password,\n",
    "            database=database\n",
    "        )\n",
    "        \n",
    "        # Create cursor\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Iterate through the DataFrame rows and insert data\n",
    "        for index, row in df.iterrows():\n",
    "            # Prepare the INSERT IGNORE query\n",
    "            query = f\"\"\"\n",
    "            INSERT IGNORE INTO job_data (job_id, job_title, location, Languages, Technologies, Skills, Education)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            \n",
    "            # Execute the query for each row\n",
    "            cursor.execute(query, (row['job_id'], row['job_title'], row['location'], row['Languages'], row['Technologies'], row['Skills'], row['Education']))\n",
    "        \n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "\n",
    "        print(\"Data inserted successfully!\")\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "    finally:\n",
    "        # Close the cursor and connection\n",
    "        if conn.is_connected():\n",
    "            cursor.close()\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dfc7c2-9d7c-4c0f-a4dc-7bb66ebcf287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data Scientist', 'Data Analyst', 'AI Engineer']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select Job title: 1, 2, 3 : 2\n",
      "Enter number of Jobs to Scrap:  50\n",
      "MySQL password:  lueQJLxezKCcgHcgCQWsPCwhnhtCPWcU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See More Jobs clicked !\n",
      "See More Jobs clicked !\n",
      "Job cards loaded :  60\n",
      "Progress: 0/50\n",
      "Progress: 1/50\n",
      "Progress: 2/50\n",
      "Progress: 3/50\n",
      "Progress: 4/50\n",
      "Progress: 5/50\n",
      "Progress: 6/50\n",
      "Progress: 7/50\n",
      "Progress: 8/50\n",
      "Progress: 9/50\n",
      "Progress: 10/50\n",
      "Progress: 11/50\n",
      "Progress: 12/50\n",
      "Progress: 13/50\n",
      "Progress: 14/50\n",
      "Progress: 15/50\n",
      "Progress: 16/50\n",
      "Progress: 17/50\n",
      "Progress: 18/50\n",
      "Progress: 19/50\n",
      "Progress: 20/50\n",
      "Progress: 21/50\n",
      "Can not click see more button\n",
      "Can not click see more button\n",
      "Can not click see more button\n",
      "Can not click see more button\n",
      "Can not click see more button\n",
      "Progress: 22/50\n",
      "Progress: 23/50\n",
      "Progress: 24/50\n",
      "Progress: 25/50\n",
      "Progress: 26/50\n",
      "Progress: 27/50\n",
      "Progress: 28/50\n",
      "Progress: 29/50\n",
      "Progress: 30/50\n",
      "Progress: 31/50\n",
      "Progress: 32/50\n",
      "Progress: 33/50\n",
      "Progress: 34/50\n",
      "Progress: 35/50\n",
      "Progress: 36/50\n",
      "Progress: 37/50\n",
      "Progress: 38/50\n",
      "Progress: 39/50\n",
      "Progress: 40/50\n",
      "Progress: 41/50\n",
      "Progress: 42/50\n",
      "Progress: 43/50\n",
      "Progress: 44/50\n",
      "Progress: 45/50\n",
      "Progress: 46/50\n",
      "Progress: 47/50\n",
      "Progress: 48/50\n",
      "Progress: 49/50\n",
      "Progress: 50/50\n"
     ]
    }
   ],
   "source": [
    "title=[\"Data Scientist\", \"Data Analyst\", \"AI Engineer\"]\n",
    "table=[\"data_scientist\", \"data_analyst\", \"ai_engineer\"]\n",
    "print(title)\n",
    "role=int(input(\"Select Job title: 1, 2, 3 :\"))\n",
    "num_of_jobs=int(input(\"Enter number of Jobs to Scrap: \"))\n",
    "password=input(\"MySQL password: \")\n",
    "path = \"C:/chromedriver-win64/chromedriver.exe\"\n",
    "df_raw = get_jobs(keyword=title[role-1], num_jobs=num_of_jobs, verbose=False, path=path, slp_time=5)\n",
    "df_with_keywords = keywords_extraction(df=df_raw, lang_path=\"C:/Users/PMLS/Downloads/languages.csv\", skill_path=\"C:/Users/PMLS/Downloads/skills.csv\", tech_path=\"C:/Users/PMLS/Downloads/technologies.csv\", education_path=\"C:/Users/PMLS/Downloads/education.csv\")\n",
    "df_to_MySQL(df=df_with_keywords, user=\"root\", password=password, host=\"maglev.proxy.rlwy.net\", port=\"53243\", database=\"railway\", table_name=\"job_data\")\n",
    "df_with_keywords.to_csv(f\"glassdoor_jobs_{table[role-1]}.csv\", index=False)\n",
    "print(\"CSV file generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c865122-06e8-40bb-a8fb-eca0816c3280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
